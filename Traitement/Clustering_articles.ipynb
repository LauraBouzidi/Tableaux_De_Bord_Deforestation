{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans as skkmeans\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_PMC_1702.txt\n",
      "clean_PMC_241.txt\n",
      "clean_PMC_2567.txt\n",
      "clean_PMC_2686.txt\n",
      "clean_PMC_3453.txt\n",
      "clean_PMC_4100.txt\n",
      "clean_PMC_4456.txt\n",
      "clean_PMC_4641.txt\n",
      "clean_PMC_5446.txt\n",
      "clean_PMC_5963.txt\n",
      "clean_PMC_6079.txt\n",
      "clean_PMC_9203.txt\n",
      "clean_PMC_9271.txt\n",
      "clean_PMC_9700.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/MARCU Quentin/Documents/Etudes/M1 SID/Scrapping/Clean_Articles_PMC\"\n",
    "dirs = os.listdir(path)\n",
    "data = open(\"Data_PMC.jsonl\",\"w\")\n",
    "\n",
    "for file in dirs:\n",
    "    try:\n",
    "        article = open(path + \"/\" + file, 'r')\n",
    "        content = article.read()\n",
    "        data.write(content + \"\\n\")\n",
    "    except:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workpath = \"C:/Users/MARCU Quentin/Documents/Etudes/M1 SID/Scrapping/\"\n",
    "\n",
    "data=open(workpath + \"Data_PMC.jsonl\",\"rb\")\n",
    "docs=[json.loads(s.decode(\"utf-8\")) for s in data.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = [x['content'] for x in docs]\n",
    "\n",
    "def sentense2cleanTokens(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = \"\".join([x if x.isalpha() else \" \" for x in sent])\n",
    "    sent = \" \".join(sent.split())\n",
    "    return sent\n",
    "\n",
    "cleancontent = [sentense2cleanTokens(x) for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(cleancontent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def distance(a,b):\n",
    "    return np.linalg.norm(a-b)\n",
    "\n",
    "\n",
    "def dualkmeans(docs, k, niter=10, nf=2):\n",
    "    docSize = docs[0].size\n",
    "    centroids = np.zeros((k,docSize))\n",
    "    centroid_features = np.array([random.sample([i for i in range(docSize)],k*nf)]).reshape(k,nf)\n",
    "    for i,cf in enumerate(centroid_features):\n",
    "        centroids[i,cf] = 1\n",
    "    for i in range(niter):\n",
    "        docsInCluster=defaultdict(list)\n",
    "        # Document assignation\n",
    "        for idoc,doc in enumerate(docs):\n",
    "            distances=[distance(np.dot(doc,c),np.zeros(docSize)) for c in centroids]\n",
    "            docsInCluster[np.argmax(distances)].append(idoc)\n",
    "        #Cluster redifinition\n",
    "        seencand=set()\n",
    "        for ic in docsInCluster:\n",
    "            candidates=np.argsort(np.ravel(np.average(docs[docsInCluster[ic]],0)))[::-1]\n",
    "            j=0\n",
    "            for cand in candidates:\n",
    "                if (not cand in seencand) and j<nf:\n",
    "                    centroid_features[ic,j]=cand\n",
    "                    j+=1\n",
    "                    seencand.add(cand)\n",
    "        centroids = np.zeros((k,docSize))\n",
    "        for ic,cf in enumerate(centroid_features):\n",
    "            centroids[ic,cf] = 1\n",
    "    return docsInCluster,centroid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters,centroid_features = dualkmeans(tfidf.todense(),10, nf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster label: experiments-data-analyzed-available-wrote\n",
      "\n",
      "Cluster label: soil-plant-microbial-community-effects\n",
      "\n",
      "Cluster label: climate-change-forest-carbon-changes\n",
      "\n",
      "Cluster label: study-model-used-models-code\n",
      "\n",
      "Cluster label: land-use-urban-vegetation-cover\n",
      "\n",
      "Cluster label: breeding-foraging-nest-nesting-nests\n",
      "\n",
      "Cluster label: food-environmental-production-management-sustainable\n",
      "\n",
      "Cluster label: genetic-populations-population-diversity-variation\n",
      "\n",
      "Cluster label: species-water-the-conservation-we\n",
      "\n",
      "Cluster label: article-supplementary-material-contains-authorized\n"
     ]
    }
   ],
   "source": [
    "ivocab = tfidf_vectorizer.get_feature_names()\n",
    "for i, cf in enumerate(centroid_features):\n",
    "    print(\"\\nCluster label:\",\"-\".join([ivocab[x] for x in cf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
